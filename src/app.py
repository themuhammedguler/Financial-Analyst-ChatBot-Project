import os
import streamlit as st
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# .env dosyasÄ±ndan environment variable'larÄ± yÃ¼kle
load_dotenv()

# Google API anahtarÄ±nÄ± al
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    st.error("Google API anahtarÄ± bulunamadÄ±. LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
    st.stop()


# VektÃ¶r veritabanÄ±nÄ±n yolu
DB_FAISS_PATH = 'vectorstore/db_faiss'

# --- RAG MÄ°MARÄ°SÄ°NÄ°N TEMEL FONKSÄ°YONLARI ---

def get_vector_store():
    """
    Daha Ã¶nce oluÅŸturulmuÅŸ ve kaydedilmiÅŸ FAISS vektÃ¶r veritabanÄ±nÄ± yÃ¼kler.
    Embedding modeli olarak Gemini'yi kullanÄ±r.
    """
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
        return vector_db
    except Exception as e:
        st.error(f"VektÃ¶r veritabanÄ± yÃ¼klenirken bir hata oluÅŸtu: {e}")
        return None

def get_conversational_chain():
    """
    Soru-cevaplama zincirini (QA Chain) oluÅŸturur. Bu zincir, LLM'in (Gemini Pro)
    nasÄ±l davranacaÄŸÄ±nÄ± belirleyen bir prompt ÅŸablonu kullanÄ±r.
    """
    
    # LLM'e verilecek talimatlarÄ± iÃ§eren prompt ÅŸablonu
    prompt_template = """
    Sen bir Finansal Analist AsistanÄ±sÄ±n. GÃ¶revin, sana verilen baÄŸlamdaki (context) finansal raporlarÄ±
    analiz ederek sorulan sorulara doÄŸru ve net cevaplar vermektir.
    
    BaÄŸlam (Context):
    {context}
    
    Soru:
    {question}
    
    Cevap:
    - CevabÄ±nÄ± sadece ve sadece sana verilen baÄŸlamdaki bilgilere dayanarak oluÅŸtur.
    - EÄŸer baÄŸlamda sorunun cevabÄ± yoksa, "Bu bilgiye sahip deÄŸilim." veya "Raporlarda bu konu hakkÄ±nda bir bilgi bulamadÄ±m." ÅŸeklinde cevap ver. Kesinlikle baÄŸlam dÄ±ÅŸÄ± bilgi kullanma veya tahmin yÃ¼rÃ¼tme.
    - CevaplarÄ±nÄ± madde iÅŸaretleri kullanarak veya kÄ±sa paragraflar halinde, kolay okunabilir bir formatta sun.
    - Finansal terimleri basit bir dille aÃ§Ä±kla.
    """
    
    # LLM modelini baÅŸlat (Gemini Pro)
    model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)
    
    # Prompt ÅŸablonunu ve modeli kullanarak bir QA zinciri oluÅŸtur
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    
    return chain

# --- STREAMLIT WEB ARAYÃœZÃœ ---

st.set_page_config(page_title="Finansal Analist Chatbot", page_icon="ğŸ“ˆ")
st.header("ğŸ“ˆ Finansal Analist Chatbot")
st.write("Akbank, THY ve TÃ¼praÅŸ'Ä±n finansal raporlarÄ±nÄ± kullanarak sorularÄ±nÄ±zÄ± yanÄ±tlar.")

# VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
vector_store = get_vector_store()

if vector_store:
    # KullanÄ±cÄ±dan soru al
    user_question = st.text_input(
        "LÃ¼tfen sorunuzu buraya yazÄ±n:",
        placeholder="Akbank'Ä±n dijitalleÅŸme vizyonu hakkÄ±nda bilgi verir misin?" # Ã–rnek metni ekledik
    )

    if user_question:
        # VektÃ¶r veritabanÄ±ndan ilgili dokÃ¼manlarÄ± bul (Similarity Search)
        # Bu aÅŸama, RAG'in "Retrieval" (Getirme) kÄ±smÄ±dÄ±r.
        docs = vector_store.similarity_search(user_question, k=5) # En alakalÄ± 5 parÃ§ayÄ± getir
        
        # Soru-cevap zincirini al
        chain = get_conversational_chain()
        
        # Zinciri Ã§alÄ±ÅŸtÄ±rarak cevabÄ± Ã¼ret
        # Bu aÅŸama, RAG'in "Augmented Generation" (ZenginleÅŸtirilmiÅŸ Ãœretim) kÄ±smÄ±dÄ±r.
        with st.spinner("Cevap oluÅŸturuluyor..."):
            response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
            st.write("### Cevap")
            st.write(response["output_text"])

        # CevabÄ±n hangi kaynaklardan Ã¼retildiÄŸini gÃ¶ster
        with st.expander("Cevap iÃ§in kullanÄ±lan kaynak metinleri gÃ¶rmek iÃ§in tÄ±klayÄ±n"):
            for i, doc in enumerate(docs):
                st.write(f"**Kaynak {i+1} (Sayfa: {doc.metadata.get('page', 'Bilinmiyor')})**")
                st.write(doc.page_content)
                st.write("---")
else:
    st.warning("UygulamanÄ±n baÅŸlayabilmesi iÃ§in vektÃ¶r veritabanÄ±nÄ±n baÅŸarÄ±yla yÃ¼klenmesi gerekmektedir.")