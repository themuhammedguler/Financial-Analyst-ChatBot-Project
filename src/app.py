import os
import streamlit as st
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# .env dosyasÄ±ndan environment variable'larÄ± yÃ¼kle
load_dotenv()

# Google API anahtarÄ±nÄ± al
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    st.error("Google API anahtarÄ± bulunamadÄ±. LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
    st.stop()


# VektÃ¶r veritabanÄ±nÄ±n yolu
DB_FAISS_PATH = 'vectorstore/db_faiss'

# --- RAG MÄ°MARÄ°SÄ°NÄ°N TEMEL FONKSÄ°YONLARI ---

def get_vector_store():
    """
    Daha Ã¶nce oluÅŸturulmuÅŸ ve kaydedilmiÅŸ FAISS vektÃ¶r veritabanÄ±nÄ± yÃ¼kler.
    Embedding modeli olarak Gemini'yi kullanÄ±r.
    """
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
        return vector_db
    except Exception as e:
        st.error(f"VektÃ¶r veritabanÄ± yÃ¼klenirken bir hata oluÅŸtu: {e}")
        return None

def get_conversational_chain():
    """
    Soru-cevaplama zincirini (QA Chain) oluÅŸturur. Bu zincir, LLM'in (Gemini Pro)
    nasÄ±l davranacaÄŸÄ±nÄ± belirleyen bir prompt ÅŸablonu kullanÄ±r.
    """
    
    # LLM'e verilecek talimatlarÄ± iÃ§eren prompt ÅŸablonu
    prompt_template = """
    Sen bir Finansal Analist AsistanÄ±sÄ±n. GÃ¶revin, sana verilen baÄŸlamdaki (context) finansal raporlarÄ±
    analiz ederek sorulan sorulara doÄŸru ve net cevaplar vermektir.
    
    BaÄŸlam (Context):
    {context}
    
    Soru:
    {question}
    
    Cevap:
    - CevabÄ±nÄ± sadece ve sadece sana verilen baÄŸlamdaki bilgilere dayanarak oluÅŸtur.
    - EÄŸer baÄŸlamda sorunun cevabÄ± yoksa, "Bu bilgiye sahip deÄŸilim." veya "Raporlarda bu konu hakkÄ±nda bir bilgi bulamadÄ±m." ÅŸeklinde cevap ver. Kesinlikle baÄŸlam dÄ±ÅŸÄ± bilgi kullanma veya tahmin yÃ¼rÃ¼tme.
    - CevaplarÄ±nÄ± madde iÅŸaretleri kullanarak veya kÄ±sa paragraflar halinde, kolay okunabilir bir formatta sun.
    - Finansal terimleri basit bir dille aÃ§Ä±kla.
    """
    
    # LLM modelini baÅŸlat (Gemini Pro)
    model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)
    
    # Prompt ÅŸablonunu ve modeli kullanarak bir QA zinciri oluÅŸtur
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    
    return chain

# --- STREAMLIT CHATBOT ARAYÃœZÃœ ---

st.set_page_config(page_title="Finansal Analist AsistanÄ±", page_icon="ğŸ“ˆ", layout="wide")
st.title("ğŸ“ˆ Finansal Analist AsistanÄ±")
st.divider()

# VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
vector_store = get_vector_store()

if not vector_store:
    st.warning("UygulamanÄ±n baÅŸlayabilmesi iÃ§in vektÃ¶r veritabanÄ±nÄ±n baÅŸarÄ±yla yÃ¼klenmesi gerekmektedir.")
    st.stop()

# AdÄ±m 1: Sohbet geÃ§miÅŸini baÅŸlatma (DEÄÄ°ÅTÄ°RÄ°LMÄ°Å Ä°LK MESAJ Ä°LE)
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", 
         "content": "Merhaba! Ben sizin Finansal Analist AsistanÄ±nÄ±zÄ±m. **Akbank, THY ve TÃ¼praÅŸ**'a ait **finansal tablolar ve yÄ±llÄ±k faaliyet raporlarÄ±** hakkÄ±nda istediÄŸiniz soruyu sorabilirsiniz."}
    ]

# AdÄ±m 2: GeÃ§miÅŸteki tÃ¼m mesajlarÄ± ekrana yazdÄ±rma
# Bu dÃ¶ngÃ¼, her etkileÅŸimde yeniden Ã§alÄ±ÅŸarak geÃ§miÅŸi ekranda tutar.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# AdÄ±m 3: KullanÄ±cÄ±dan yeni bir girdi alma
# st.chat_input, sayfanÄ±n altÄ±nda sabit bir giriÅŸ kutusu oluÅŸturur.
if prompt := st.chat_input("Akbank'Ä±n dijitalleÅŸme vizyonu hakkÄ±nda bilgi verir misin?"):
    
    # a. KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± sohbet geÃ§miÅŸine ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # b. KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± ekrana anÄ±nda yazdÄ±r
    with st.chat_message("user"):
        st.markdown(prompt)

    # c. AsistanÄ±n cevabÄ±nÄ± oluÅŸturma (RAG sÃ¼reci)
    with st.chat_message("assistant"):
        with st.spinner("Analiz ediliyor..."):
            # RAG zincirini Ã§alÄ±ÅŸtÄ±r (eski kodunuzdaki mantÄ±ÄŸÄ±n aynÄ±sÄ±)
            docs = vector_store.similarity_search(prompt, k=5)
            chain = get_conversational_chain()
            response = chain({"input_documents": docs, "question": prompt}, return_only_outputs=True)
            
            # CevabÄ± ve kaynaklarÄ± ekrana yazdÄ±r
            assistant_response = response["output_text"]
            st.markdown(assistant_response)

            # KaynaklarÄ± cevabÄ±n altÄ±na gizlenmiÅŸ bir ÅŸekilde ekle
            with st.expander("Referans AlÄ±nan Kaynak Metinleri GÃ¶r"):
                for i, doc in enumerate(docs):
                    source_filename = os.path.basename(doc.metadata.get('source', 'Bilinmiyor'))
                    st.info(f"**Kaynak {i+1}** | Dosya: `{source_filename}` | Sayfa: `{doc.metadata.get('page', 'Bilinmiyor')}`")
                    st.text_area(label="", value=doc.page_content, height=150, key=f"expander_source_{i}")

    # d. AsistanÄ±n cevabÄ±nÄ± da sohbet geÃ§miÅŸine ekle
    st.session_state.messages.append({"role": "assistant", "content": assistant_response})